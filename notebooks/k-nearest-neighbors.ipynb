{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261a3062-5878-4708-b4f9-fb147f011740",
   "metadata": {},
   "source": [
    "# k Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a8f9d5-0a04-4767-a03b-cadca477ed19",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b231ac-51de-4db7-a5e9-1b06881904b5",
   "metadata": {},
   "source": [
    "The k-nearest neighbors algorithm is a simple and intuitive classification algorithm. It operates on the assumption that data points with similar features tend to belong to the same class. The algorithm classifies a new data point by finding the k nearest neighbors to that point in the feature space and assigning the class label that is most common among its neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724f114-4198-4aa8-a297-f6f93486e4d1",
   "metadata": {},
   "source": [
    "Let's consider a simple example of classifying fruits based on their weight and color. We have a dataset of fruits, each labeled as either \"apple\" or \"orange,\" along with their corresponding weight and color. To classify a new fruit, we calculate the distances between the new fruit and all fruits in the dataset. Suppose the new fruit is a small, red fruit. We choose a value for k, let's say k=3, and find the three nearest neighbors to the new fruit. If two of the nearest neighbors are apples and one is an orange, we classify the new fruit as an apple since apples are the majority among its nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ce9a4-0f60-46c9-8573-3ba283cad1bc",
   "metadata": {},
   "source": [
    "Here's a simple ASCII representation of the k-NN algorithm in action:\n",
    "\n",
    "```mathematica\n",
    "Training Dataset:\n",
    "          Blue (B)        Red (R)\n",
    "    ------------------------------\n",
    "    1 |    B                R\n",
    "    2 |       B          R\n",
    "    3 |          B    R\n",
    "    4 |    B        R\n",
    "    5 |       B   R\n",
    "\n",
    "New Data Point: (X)\n",
    "    ------------------\n",
    "    6 |      X\n",
    "\n",
    "K = 3 (Nearest Neighbors)\n",
    "```\n",
    "\n",
    "In this example, we have a training dataset with blue and red data points. We want to classify a new data point (represented by \"X\") based on its k nearest neighbors. By measuring the distances from the new data point to all other data points, we can identify the k nearest neighbors and assign the class label based on the majority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15f1015-b877-4599-9782-14889b9008a4",
   "metadata": {},
   "source": [
    "The space complexity of the k-NN algorithm is relatively low. During the training phase, the algorithm stores the training dataset, requiring space proportional to the number of data points and the number of features. During the prediction phase, the space complexity is minimal as it only needs to store the new data point and the k nearest neighbors.\n",
    "\n",
    "The time complexity of the k-NN algorithm can be higher than other algorithms during the prediction phase, especially when dealing with large datasets. For each new data point, it needs to calculate the distances to all training data points, which takes O(n) time, where n is the number of data points. Additionally, identifying the k nearest neighbors requires sorting or searching, which can take O(n log n) or O(n) time, respectively, depending on the implementation.\n",
    "\n",
    "Both the iterative and recursive versions of the k-NN algorithm have similar space and time complexities since the algorithm mainly relies on distance calculations and finding nearest neighbors, rather than iterative or recursive operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1649bb-2b47-4df4-b03b-27b8f93e9278",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648475a-96fc-46be-abfd-a8c8ea329bbb",
   "metadata": {},
   "source": [
    "Two things:\n",
    "- The implementation is sensitive to the format of your data.\n",
    "- kNN can solve regression, as well, by averaging the values of its k nearest neighbors along the dimension to predict. Here, I've only implemented classification, beause laziness (and CPP matters more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75dfacdc-1d78-41a6-b710-73dfcb5a61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import pathlib\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba1506ad-cacf-4860-b0e9-7cafaf47c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k: int, data_path: pathlib.Path):\n",
    "        self.k = int\n",
    "        self.data_path = data_path\n",
    "        self.data: List[Dict[str, Union[float, str]]] = self.__load_data()\n",
    "        self.stripped: List[List[float]] = self.__strip_data()\n",
    "        \n",
    "    def __load_data(self) -> List[Dict[str, Union[str, float]]]:\n",
    "        \"\"\"Load and attach CSV data to KNN instance.\"\"\"\n",
    "        with open(self.data_path, mode=\"r\") as data:\n",
    "            reader = csv.DictReader(data)\n",
    "            self.data = [row for row in reader]\n",
    "        return self.data\n",
    "    \n",
    "    def __strip_data(self) -> List[List[float]]:\n",
    "        \"\"\"Remove `label` from each entry, and strip keys to create pure list representation.\"\"\"\n",
    "        self.stripped = []\n",
    "        for point in self.data:\n",
    "            temp = []\n",
    "            for key, value in point.items():\n",
    "                if key == \"label\":\n",
    "                    continue\n",
    "                temp.append(float(value))\n",
    "            self.stripped.append(temp)\n",
    "        return self.stripped\n",
    "        \n",
    "    def distance(self, point1: List[float], point2: List[float]) -> float:\n",
    "        \"\"\"Compute Euclidean distance from `point1` to `point2`.\"\"\"\n",
    "        return math.sqrt(sum([x ** 2 + y ** 2 for x, y in zip(point1, point2)]))\n",
    "        \n",
    "    def nearest(self) -> List[Dict[str, Union[str, float]]]:\n",
    "        \"\"\"...\"\"\"\n",
    "        # Sort on `distance`\n",
    "        sorted(self.data, key=lambda point: point[\"distance\"])\n",
    "        return self.data[:self.k]\n",
    "        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def find_majority(self, nearest: List[Dict[str, Union[str, float]]]) -> Tuple[str, str]:\n",
    "        \"\"\"Identifies indices of nearest neighbors and returns majority label.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def classify(self, point: List[float]):\n",
    "        \"\"\"...\"\"\"\n",
    "        # Compute distance from `point` to everything in `self.data`\n",
    "        distances = [distance(point, p2) for p2 in self.stripped]\n",
    "        # Compute/return average neighbor value\n",
    "        return self.find_majority(self.nearest())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b55d0f49-62f3-443f-bbd8-0d12a747c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNN(k=3, data_path=\"knn.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
