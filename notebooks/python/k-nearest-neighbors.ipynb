{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261a3062-5878-4708-b4f9-fb147f011740",
   "metadata": {},
   "source": [
    "# k Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a8f9d5-0a04-4767-a03b-cadca477ed19",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b231ac-51de-4db7-a5e9-1b06881904b5",
   "metadata": {},
   "source": [
    "The k-nearest neighbors algorithm is a simple and intuitive classification algorithm. It operates on the assumption that data points with similar features tend to belong to the same class. The algorithm classifies a new data point by finding the k nearest neighbors to that point in the feature space and assigning the class label that is most common among its neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724f114-4198-4aa8-a297-f6f93486e4d1",
   "metadata": {},
   "source": [
    "Let's consider a simple example of classifying fruits based on their weight and color. We have a dataset of fruits, each labeled as either \"apple\" or \"orange,\" along with their corresponding weight and color. To classify a new fruit, we calculate the distances between the new fruit and all fruits in the dataset. Suppose the new fruit is a small, red fruit. We choose a value for k, let's say k=3, and find the three nearest neighbors to the new fruit. If two of the nearest neighbors are apples and one is an orange, we classify the new fruit as an apple since apples are the majority among its nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ce9a4-0f60-46c9-8573-3ba283cad1bc",
   "metadata": {},
   "source": [
    "Here's a simple ASCII representation of the k-NN algorithm in action:\n",
    "\n",
    "```mathematica\n",
    "Training Dataset:\n",
    "          Blue (B)        Red (R)\n",
    "    ------------------------------\n",
    "    1 |    B                R\n",
    "    2 |       B          R\n",
    "    3 |          B    R\n",
    "    4 |    B        R\n",
    "    5 |       B   R\n",
    "\n",
    "New Data Point: (X)\n",
    "    ------------------\n",
    "    6 |      X\n",
    "\n",
    "K = 3 (Nearest Neighbors)\n",
    "```\n",
    "\n",
    "In this example, we have a training dataset with blue and red data points. We want to classify a new data point (represented by \"X\") based on its k nearest neighbors. By measuring the distances from the new data point to all other data points, we can identify the k nearest neighbors and assign the class label based on the majority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15f1015-b877-4599-9782-14889b9008a4",
   "metadata": {},
   "source": [
    "The space complexity of the k-NN algorithm is relatively low. During the training phase, the algorithm stores the training dataset, requiring space proportional to the number of data points and the number of features. During the prediction phase, the space complexity is minimal as it only needs to store the new data point and the k nearest neighbors.\n",
    "\n",
    "The time complexity of the k-NN algorithm can be higher than other algorithms during the prediction phase, especially when dealing with large datasets. For each new data point, it needs to calculate the distances to all training data points, which takes O(n) time, where n is the number of data points. Additionally, identifying the k nearest neighbors requires sorting or searching, which can take O(n log n) or O(n) time, respectively, depending on the implementation.\n",
    "\n",
    "Both the iterative and recursive versions of the k-NN algorithm have similar space and time complexities since the algorithm mainly relies on distance calculations and finding nearest neighbors, rather than iterative or recursive operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1649bb-2b47-4df4-b03b-27b8f93e9278",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648475a-96fc-46be-abfd-a8c8ea329bbb",
   "metadata": {},
   "source": [
    "Two things:\n",
    "- The implementation is, as usual, sensitive to the format of your data.\n",
    "- kNN can solve regression, as well, by averaging the values of its k nearest neighbors along the dimension to predict. Here, I've only implemented classification, beause laziness (and CPP matters more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "75dfacdc-1d78-41a6-b710-73dfcb5a61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ba1506ad-cacf-4860-b0e9-7cafaf47c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k: int, data_path: pathlib.Path):\n",
    "        self.k = k\n",
    "        self.data_path = data_path\n",
    "        self.data: List[Dict[str, Union[float, str]]] = self.__load_data()\n",
    "        self.transformed_data: List[Dict[str, Union[float, str]]] = self.__transform_data()\n",
    "        self.neartest: List[Dict[str, Union[float, str]]] = [] \n",
    "        \n",
    "    def __load_data(self) -> List[Dict[str, Union[str, float]]]:\n",
    "        \"\"\"Load and attach CSV data to KNN instance.\"\"\"\n",
    "        with open(self.data_path, mode=\"r\") as data:\n",
    "            reader = csv.DictReader(data)\n",
    "            self.data = [row for row in reader]\n",
    "        return self.data\n",
    "\n",
    "    def __transform_data(self) -> List[Dict[str, Union[str, float]]]:\n",
    "        \"\"\"...\"\"\"\n",
    "        self.transformed_data = []\n",
    "        for element in self.data:\n",
    "            self.transformed_data.append({\n",
    "                \"label\": element[\"label\"],\n",
    "                \"features\": tuple(float(v) for k, v in element.items() if k != \"label\"),\n",
    "            })\n",
    "        return self.transformed_data\n",
    "        \n",
    "    def compute_distances(self, point: Tuple[float, float]) -> List[Dict[str, Union[float, str]]]:\n",
    "        \"\"\"Compute Euclidean distance from `point1` to `point2`.\"\"\"\n",
    "        for element in self.transformed_data:\n",
    "            element[\"distance\"] = np.linalg.norm(np.array(point)-np.array(element[\"features\"]))\n",
    "        return self.transformed_data\n",
    "        \n",
    "    def find_nearest(self) -> \"KNN\":\n",
    "        \"\"\"Sort `self.transformed_data` on `distance`, and return the k nearest neighbors found.\"\"\"\n",
    "        self.transformed_data = sorted(self.transformed_data, key=lambda element: element[\"distance\"])\n",
    "        self.nearest = self.transformed_data[:self.k]\n",
    "        return self.nearest\n",
    "        \n",
    "    def classify(self, label_only: bool=False) -> Tuple[str, float]:\n",
    "        \"\"\"Identifies indices of nearest neighbors and returns majority label.\"\"\"\n",
    "        for element in self.nearest:\n",
    "            if not element.get(\"label_count\"):\n",
    "                element[\"label_count\"] = 1\n",
    "            else:\n",
    "                element[\"label_count\"] += 1\n",
    "\n",
    "        sorted(self.nearest, key=lambda k: k[\"label_count\"])\n",
    "        return self.nearest[0][\"label\"] if label_only else self.nearest[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359cf451-a2c2-4acf-b62d-939608586ed9",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7f5982bd-1386-4718-961b-5f528998b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"knn.csv\"\n",
    "k = 10\n",
    "point = (200, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9b37b70c-9bc4-4c98-8ff9-e37667eaad57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNN(k=k, data_path=data_path)\n",
    "model.compute_distances(point)\n",
    "model.find_nearest()\n",
    "model.classify(label_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b836268f-7772-4041-8f36-a407bd1cc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "point = (80, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9dfb0c9d-a98d-4a0d-ad76-bd55d5a87f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'orange'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNN(k=k, data_path=data_path)\n",
    "model.compute_distances(point)\n",
    "model.find_nearest()\n",
    "model.classify(label_only=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
